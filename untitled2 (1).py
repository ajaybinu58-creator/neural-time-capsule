# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOAkxElxZv0oyrVy7SqKq8vLGg1RLJTp
"""

from google.colab import drive
drive.mount('/content/drive')

import os, glob, warnings
warnings.filterwarnings("ignore")

# Set paths
BASE_PATH = "/content/drive/MyDrive/NTC_Datasets"
os.makedirs("/content/processed", exist_ok=True)

# Verify
print("Files found:", os.listdir(BASE_PATH))

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from scipy.signal import welch
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import joblib, json, os

def compute_window_features(df, col_prefix):
    cols = [c for c in df.columns if c.startswith('val_') or c.isdigit()]
    arr = df[cols].astype(float).replace([np.inf, -np.inf], np.nan)
    arr = arr.loc[arr.isna().mean(axis=1) < 0.5]  # drop too-empty rows

    out = pd.DataFrame(index=arr.index)
    vals = arr.values

    # Basic statistics
    out[f'{col_prefix}_mean'] = np.nanmean(vals, axis=1)
    out[f'{col_prefix}_std']  = np.nanstd(vals, axis=1)
    out[f'{col_prefix}_min']  = np.nanmin(vals, axis=1)
    out[f'{col_prefix}_max']  = np.nanmax(vals, axis=1)
    out[f'{col_prefix}_range'] = out[f'{col_prefix}_max'] - out[f'{col_prefix}_min']
    out[f'{col_prefix}_iqr'] = np.nanpercentile(vals,75,axis=1) - np.nanpercentile(vals,25,axis=1)
    out[f'{col_prefix}_rms'] = np.sqrt(np.nanmean(vals**2, axis=1))

    # Trend slope
    idx = np.arange(vals.shape[1]).reshape(-1,1)
    slopes = []
    for row in vals:
        mask = ~np.isnan(row)
        if mask.sum() < 2: slopes.append(np.nan); continue
        lr = LinearRegression().fit(idx[mask], row[mask])
        slopes.append(lr.coef_[0])
    out[f'{col_prefix}_slope'] = slopes

    # Zero-crossing rate
    zcr = []
    for row in vals:
        row = row - np.nanmean(row)
        mask = ~np.isnan(row)
        if mask.sum() < 2: zcr.append(np.nan); continue
        s = np.sign(row[mask])
        zcr.append(((s[:-1]*s[1:])<0).sum())
    out[f'{col_prefix}_zcr'] = zcr

    # Power spectral density
    psd = []
    for row in vals:
        rr = row[~np.isnan(row)]
        if len(rr) < 4: psd.append(np.nan); continue
        f, Pxx = welch(rr, nperseg=min(8,len(rr)))
        psd.append(np.nanmax(Pxx))
    out[f'{col_prefix}_max_psd'] = psd

    out[f'{col_prefix}_skew'] = skew(vals, axis=1, nan_policy='omit')
    out[f'{col_prefix}_kurtosis'] = kurtosis(vals, axis=1, nan_policy='omit')

    if 'label' in df.columns:
        out['label'] = df['label']
    return out.reset_index(drop=True)

physio_files = ["heartrate.csv", "spo2.csv", "skin_temperature.csv"]
physio_features = {}

for f in physio_files:
    path = os.path.join(BASE_PATH, f)
    if not os.path.exists(path): continue
    df = pd.read_csv(path)
    name = f.split('.')[0]
    feats = compute_window_features(df, name)
    feats.to_csv(f"/content/processed/{name}_features.csv", index=False)
    physio_features[name] = feats
    print(f"{name}: {feats.shape}")

def filter_range(df, col, low, high):
    return df[(df[col] >= low) & (df[col] <= high)].reset_index(drop=True)

if "heartrate" in physio_features:
    hr = physio_features["heartrate"]
    hr = filter_range(hr, "heartrate_mean", 40, 180)
    hr.to_csv("/content/processed/heartrate_clean.csv", index=False)

if "spo2" in physio_features:
    spo = physio_features["spo2"]
    spo = filter_range(spo, "spo2_mean", 80, 100)
    spo.to_csv("/content/processed/spo2_clean.csv", index=False)

if "skin_temperature" in physio_features:
    sk = physio_features["skin_temperature"]
    sk = filter_range(sk, "skin_temperature_mean", 28, 42)
    sk.to_csv("/content/processed/skin_temp_clean.csv", index=False)

df_em = pd.read_csv(f"{BASE_PATH}/Emotional_combined.csv")

# Example mappings
likert_map_5 = {
 'Disagree strongly': 1, 'Disagree a little': 2, 'Neither agree nor disagree': 3,
 'Agree a little': 4, 'Agree strongly': 5
}
pss_map = {'Never':0,'Almost never':1,'Sometimes':2,'Fairly often':3,'Very often':4}
panas_map = {'Very slightly or not at all':1, 'A little':2, 'Moderately':3, 'Quite a bit':4, 'Extremely':5}

for col in df_em.columns:
    if df_em[col].dtype == object:
        df_em[col] = df_em[col].map(likert_map_5).fillna(df_em[col].map(pss_map)).fillna(df_em[col].map(panas_map))

# Drop too-empty columns
df_em = df_em.loc[:, df_em.isna().mean() < 0.5]

# Impute
imputer = SimpleImputer(strategy="median")
df_em[df_em.columns] = imputer.fit_transform(df_em)

df_em.to_csv("/content/processed/emotion_imputed.csv", index=False)

pos = [c for c in df_em.columns if "enthusiastic" in c or "active" in c or "proud" in c]
neg = [c for c in df_em.columns if "distressed" in c or "upset" in c or "guilty" in c]

df_em["mood_score"] = (df_em[pos].mean(axis=1) - df_em[neg].mean(axis=1))
df_em["mood_score"] = (df_em["mood_score"] - df_em["mood_score"].min()) / (df_em["mood_score"].max() - df_em["mood_score"].min())

pss_cols = [c for c in df_em.columns if "pss" in c.lower()]
df_em["stress_level"] = (df_em[pss_cols].mean(axis=1) - df_em[pss_cols].min().min()) / (df_em[pss_cols].max().max() - df_em[pss_cols].min().min())

energy_cols = [c for c in df_em.columns if "active" in c or "determined" in c]
df_em["energy_level"] = (df_em[energy_cols].mean(axis=1) - df_em[energy_cols].min().min()) / (df_em[energy_cols].max().max() - df_em[energy_cols].min().min())

df_em["focus_level"] = 1 - df_em["stress_level"]

emotion_features = df_em[["mood_score","stress_level","energy_level","focus_level"]]
emotion_features.to_csv("/content/processed/emotion_features.csv", index=False)

import pandas as pd, numpy as np
start = pd.Timestamp("2025-10-01 00:00")
for f in ["heartrate_clean","spo2_clean","skin_temp_clean"]:
    path = f"/content/processed/{f}.csv"
    if os.path.exists(path):
        df = pd.read_csv(path)
        df["timestamp"] = start + pd.to_timedelta(np.arange(len(df)), unit="m")
        df.to_csv(path.replace(".csv","_ts.csv"), index=False)

# Aggregate physio to daily
dfs = []
for f in ["heartrate_clean_ts.csv","spo2_clean_ts.csv","skin_temp_clean_ts.csv"]:
    path = f"/content/processed/{f}"
    if os.path.exists(path):
        d = pd.read_csv(path, parse_dates=["timestamp"])
        d["date"] = d["timestamp"].dt.date
        # Select only numeric columns before grouping and averaging
        numeric_cols = d.select_dtypes(include=np.number).columns
        dfs.append(d.groupby("date")[numeric_cols].mean().reset_index())

physio_daily = dfs[0]
for df in dfs[1:]:
    physio_daily = physio_daily.merge(df, on="date", how="outer")

emotion_features["date"] = pd.to_datetime(pd.date_range(start="2025-10-01", periods=len(emotion_features), freq="D")).date
merged = physio_daily.merge(emotion_features, on="date", how="inner")
merged.to_csv("/content/processed/merged_dataset.csv", index=False)

print("Merged dataset shape:", merged.shape)

scaler = StandardScaler()
num_cols = merged.select_dtypes(include=np.number).columns
merged[num_cols] = scaler.fit_transform(merged[num_cols])
joblib.dump(scaler, "/content/processed/scaler.pkl")
merged.to_csv("/content/processed/final_scaled_dataset.csv", index=False)
print("✅ Saved final scaled dataset")

print(merged.head())
print("\nSummary statistics:\n", merged.describe().T)
merged[num_cols].hist(figsize=(12,8))
plt.show()

import pandas as pd
df = pd.read_csv('/content/processed/final_scaled_dataset.csv')
print(df.shape)
df.head()

"""# Task
Integrate additional data sources (digital behavior, contextual, and environmental) into the existing physiological and emotional dataset, process and merge them into a single daily-aggregated DataFrame, handle missing values, scale the final dataset, and save it. Use the files "app_usage_combined.csv", "Behavior_combined.csv", and "calendar_combined.csv".

## Load and process digital behavior data

### Subtask:
Load and process digital behavior data
```
## Subtask
Load the `app_usage_combined.csv` and `Behavior_combined.csv` files, clean and extract relevant features (e.g., total screen time, time spent in different app categories, number of unlocks). Aggregate this data to a daily level.

### Instructions
1. Load the `app_usage_combined.csv` file into a pandas DataFrame named `df_app_usage` from the `BASE_PATH`.
2. Convert the 'timestamp' column in `df_app_usage` to datetime objects.
3. Extract the date from the 'timestamp' column and store it in a new column named 'date' in `df_app_usage`.
4. Load the `Behavior_combined.csv` file into a pandas DataFrame named `df_behavior` from the `BASE_PATH`.
5. Convert the 'timestamp' column in `df_behavior` to datetime objects.
6. Extract the date from the 'timestamp' column and store it in a new column named 'date' in `df_behavior`.
7. For `df_app_usage`, group the DataFrame by 'date' and aggregate the relevant columns to calculate daily features. Consider metrics like the sum of 'duration' (total screen time) and potentially counts or sums related to different app categories if available in the dataset. Store the result in `df_app_usage_daily`.
8. For `df_behavior`, group the DataFrame by 'date' and aggregate the relevant columns to calculate daily features. Consider metrics like the count of 'unlocks' or other relevant behavior events. Store the result in `df_behavior_daily`.
9. Merge `df_app_usage_daily` and `df_behavior_daily` on the 'date' column, performing an outer merge to keep all dates from both dataframes. Store the result in `df_digital_behavior_daily`.
10. Save the `df_digital_behavior_daily` DataFrame to a CSV file named `digital_behavior_daily.csv` in the `/content/processed` directory.
```

**Reasoning**:
Load the app usage data and behavior data, convert timestamps to datetime objects, and extract the date for daily aggregation.
"""

df_app_usage = pd.read_csv(f"{BASE_PATH}/app_usage_combined.csv")
df_app_usage["timestamp"] = pd.to_datetime(df_app_usage["timestamp"])
df_app_usage["date"] = df_app_usage["timestamp"].dt.date

df_behavior = pd.read_csv(f"{BASE_PATH}/Behavior_combined.csv")
df_behavior["timestamp"] = pd.to_datetime(df_behavior["timestamp"])
df_behavior["date"] = df_behavior["timestamp"].dt.date

"""**Reasoning**:
The previous command failed because the 'timestamp' column was not found in `df_behavior`. Inspect the columns of `df_behavior` to identify the correct timestamp column name and retry the conversion and date extraction.


"""

print(df_behavior.columns)
df_behavior['resp_time'] = pd.to_datetime(df_behavior['resp_time'], unit='s')
df_behavior['date'] = df_behavior['resp_time'].dt.date

df_app_usage = pd.read_csv(f"{BASE_PATH}/app_usage_combined.csv")
df_app_usage["timestamp"] = pd.to_datetime(df_app_usage["timestamp"])
df_app_usage["date"] = df_app_usage["timestamp"].dt.date

"""**Reasoning**:
Now that the timestamp columns are correctly identified and converted to datetime objects, aggregate the dataframes by date to calculate daily features and then merge them.


"""

print(df_app_usage.columns)

df_app_usage_daily = df_app_usage.groupby("date").agg(
    num_app_events=("timestamp", "count"),
    num_running_tasks=("running_tasks_numrunning", "sum")
).reset_index()

df_behavior_daily = df_behavior.groupby("date").agg(
    num_behavior_events=("resp_time", "count")
).reset_index()

df_digital_behavior_daily = df_app_usage_daily.merge(df_behavior_daily, on="date", how="outer")
df_digital_behavior_daily.to_csv("/content/processed/digital_behavior_daily.csv", index=False)

"""**Reasoning**:
The previous command failed because the column 'running_tasks_duration' does not exist in `df_app_usage`. Inspect the columns of `df_app_usage` to identify available columns for aggregation, then proceed with the aggregation and merging steps.


"""

print(df_app_usage.columns)

df_app_usage_daily = df_app_usage.groupby("date").agg(
    num_app_events=("timestamp", "count"),
    num_running_tasks=("running_tasks_numrunning", "sum")
).reset_index()

df_behavior_daily = df_behavior.groupby("date").agg(
    num_behavior_events=("resp_time", "count")
).reset_index()

df_digital_behavior_daily = df_app_usage_daily.merge(df_behavior_daily, on="date", how="outer")
df_digital_behavior_daily.to_csv("/content/processed/digital_behavior_daily.csv", index=False)

"""## Load and process contextual data

### Subtask:
Load the `calendar_combined.csv` file, clean and extract relevant features (e.g., number of events, duration of events, tags). Aggregate this data to a daily level.

**Reasoning**:
Load the calendar data and inspect its columns to identify relevant ones for processing.
"""

df_calendar = pd.read_csv(f"{BASE_PATH}/calendar_combined.csv")
print(df_calendar.columns)
display(df_calendar.head())

"""**Reasoning**:
Convert the existing 'date' column to datetime objects and aggregate the data to daily level by counting the number of events per day.


"""

df_calendar['date'] = pd.to_datetime(df_calendar['date'], format='%m/%d/%Y', errors='coerce').dt.date
df_calendar_daily = df_calendar.groupby('date').size().reset_index(name='num_calendar_events')
display(df_calendar_daily.head())

"""**Reasoning**:
Save the daily aggregated calendar data to a CSV file.


"""

df_calendar_daily.to_csv("/content/processed/calendar_daily.csv", index=False)

"""## Load and process environmental data

### Subtask:
Load any relevant environmental data if available (e.g., weather data if you have a source). Clean and aggregate this data to a daily level.

**Reasoning**:
Check the BASE_PATH for environmental data files and load any found.
"""

environmental_files = [f for f in os.listdir(BASE_PATH) if 'environment' in f.lower() or 'weather' in f.lower()]
print("Environmental files found:", environmental_files)

environmental_df = None
if environmental_files:
    # Assuming the first environmental file found is the relevant one and it's a CSV
    environmental_file_path = os.path.join(BASE_PATH, environmental_files[0])
    try:
        environmental_df = pd.read_csv(environmental_file_path)
        print("Loaded environmental data shape:", environmental_df.shape)
        display(environmental_df.head())
    except Exception as e:
        print(f"Error loading environmental file {environmental_files[0]}: {e}")
else:
    print("No environmental data files found in BASE_PATH.")

"""**Reasoning**:
No environmental data was found, so create an empty DataFrame with a 'date' column and save it for compatibility in later merge steps.


"""

environmental_daily_df = pd.DataFrame(columns=['date'])
environmental_daily_df.to_csv("/content/processed/environmental_daily.csv", index=False)
print("Created empty environmental_daily.csv as no environmental data was found.")

"""## Merge all data sources

### Subtask:
Merge the daily aggregated data from the physiological, emotional, digital behavior, contextual, and environmental layers into a single DataFrame based on the date.

**Reasoning**:
Load the necessary daily aggregated dataframes and ensure their 'date' columns are in the correct format before merging.
"""

physio_emotion_daily = pd.read_csv("/content/processed/merged_dataset.csv", parse_dates=["date"])
df_digital_behavior_daily = pd.read_csv("/content/processed/digital_behavior_daily.csv", parse_dates=["date"])
df_calendar_daily = pd.read_csv("/content/processed/calendar_daily.csv", parse_dates=["date"])
environmental_daily_df = pd.read_csv("/content/processed/environmental_daily.csv")

# Convert date columns to datetime objects, handling potential errors
physio_emotion_daily['date'] = pd.to_datetime(physio_emotion_daily['date']).dt.date
df_digital_behavior_daily['date'] = pd.to_datetime(df_digital_behavior_daily['date']).dt.date
df_calendar_daily['date'] = pd.to_datetime(df_calendar_daily['date']).dt.date
# For the potentially empty environmental_daily_df, convert only if not empty
if not environmental_daily_df.empty:
    environmental_daily_df['date'] = pd.to_datetime(environmental_daily_df['date']).dt.date
else:
    # If empty, ensure the 'date' column dtype is object to prevent further errors
    environmental_daily_df['date'] = environmental_daily_df['date'].astype(object)


# Merge the dataframes sequentially
df_merged_daily = physio_emotion_daily.merge(df_digital_behavior_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(df_calendar_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(environmental_daily_df, on="date", how="outer")

print("Merged dataset shape:", df_merged_daily.shape)
display(df_merged_daily.head())

"""**Reasoning**:
The error indicates that the 'date' column is missing in one of the files when trying to parse dates. I need to check the columns of "emotion_features.csv" to see if the 'date' column exists.


"""

emotion_features = pd.read_csv("/content/processed/emotion_features.csv")
print(emotion_features.columns)

"""**Reasoning**:
The 'date' column is missing in `emotion_features.csv`. The previous code cells show that the `emotion_features` dataframe was created *without* a 'date' column, but a 'date' column was added to the `merged` dataframe *after* emotion features were merged with physio daily data. I need to load the `merged_dataset.csv` which contains both physio and emotion features with a date column, and then merge it with the other daily dataframes.


"""

physio_emotion_daily = pd.read_csv("/content/processed/merged_dataset.csv", parse_dates=["date"])
df_digital_behavior_daily = pd.read_csv("/content/processed/digital_behavior_daily.csv", parse_dates=["date"])
df_calendar_daily = pd.read_csv("/content/processed/calendar_daily.csv", parse_dates=["date"])
environmental_daily_df = pd.read_csv("/content/processed/environmental_daily.csv")

# Convert date columns to datetime objects, handling potential errors
physio_emotion_daily['date'] = pd.to_datetime(physio_emotion_daily['date']).dt.date
df_digital_behavior_daily['date'] = pd.to_datetime(df_digital_behavior_daily['date']).dt.date
df_calendar_daily['date'] = pd.to_datetime(df_calendar_daily['date']).dt.date
# For the potentially empty environmental_daily_df, convert only if not empty
if not environmental_daily_df.empty:
    environmental_daily_df['date'] = pd.to_datetime(environmental_daily_df['date']).dt.date
else:
    # If empty, ensure the 'date' column dtype is object to prevent further errors
    environmental_daily_df['date'] = environmental_daily_df['date'].astype(object)


# Merge the dataframes sequentially
df_merged_daily = physio_emotion_daily.merge(df_digital_behavior_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(df_calendar_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(environmental_daily_df, on="date", how="outer")

print("Merged dataset shape:", df_merged_daily.shape)

"""**Reasoning**:
The error "AttributeError: Can only use .dt accessor with datetimelike values" when accessing `environmental_daily_df['date'].dt.date` indicates that the 'date' column in this dataframe is not a datetime object. This is likely because the `environmental_daily_df` was created as an empty dataframe with a 'date' column of object dtype in a previous step. I need to handle this case by ensuring the 'date' column is converted to datetime objects before attempting to access the `.dt` accessor.


"""

physio_emotion_daily = pd.read_csv("/content/processed/merged_dataset.csv", parse_dates=["date"])
df_digital_behavior_daily = pd.read_csv("/content/processed/digital_behavior_daily.csv", parse_dates=["date"])
df_calendar_daily = pd.read_csv("/content/processed/calendar_daily.csv", parse_dates=["date"])
environmental_daily_df = pd.read_csv("/content/processed/environmental_daily.csv")

# Convert date columns to datetime objects, handling potential errors
physio_emotion_daily['date'] = pd.to_datetime(physio_emotion_daily['date']).dt.date
df_digital_behavior_daily['date'] = pd.to_datetime(df_digital_behavior_daily['date']).dt.date
df_calendar_daily['date'] = pd.to_datetime(df_calendar_daily['date']).dt.date
# For the potentially empty environmental_daily_df, convert only if not empty
if not environmental_daily_df.empty:
    environmental_daily_df['date'] = pd.to_datetime(environmental_daily_df['date']).dt.date
else:
    # If empty, ensure the 'date' column dtype is object to prevent further errors
    environmental_daily_df['date'] = environmental_daily_df['date'].astype(object)


# Merge the dataframes sequentially
df_merged_daily = physio_emotion_daily.merge(df_digital_behavior_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(df_calendar_daily, on="date", how="outer")
df_merged_daily = df_merged_daily.merge(environmental_daily_df, on="date", how="outer")

print("Merged dataset shape:", df_merged_daily.shape)
display(df_merged_daily.head())

"""## Handle missing values

### Subtask:
Address any missing values that may have been introduced during the merging process.

**Reasoning**:
Inspect the `df_merged_daily` DataFrame for missing values.
"""

print("Missing values before imputation:\n", df_merged_daily.isnull().sum())

"""**Reasoning**:
Apply median imputation to handle missing values in the numeric columns, as forward or backward fill might not be suitable for all features and median is robust to outliers. The emotional features have all missing values, which cannot be reasonably imputed with median, so they will be dropped.


"""

# Drop columns with all missing values (emotional features)
cols_to_drop = ['mood_score', 'stress_level', 'energy_level', 'focus_level']
df_merged_daily = df_merged_daily.drop(columns=cols_to_drop)

# Impute remaining numeric columns with the median
numeric_cols_after_drop = df_merged_daily.select_dtypes(include=np.number).columns
imputer = SimpleImputer(strategy='median')
df_merged_daily[numeric_cols_after_drop] = imputer.fit_transform(df_merged_daily[numeric_cols_after_drop])

print("\nMissing values after imputation:\n", df_merged_daily.isnull().sum())

"""## Final scaling and saving

### Subtask:
Apply the same scaling process used before to the new merged dataset and save the final scaled dataset.

**Reasoning**:
Apply standard scaling to the numeric columns of the merged daily dataset and save the scaled dataframe.
"""

numeric_cols = df_merged_daily.select_dtypes(include=np.number).columns
scaler = StandardScaler()
df_merged_daily[numeric_cols] = scaler.fit_transform(df_merged_daily[numeric_cols])
df_merged_daily.to_csv("/content/processed/final_merged_scaled_dataset.csv", index=False)
print("✅ Saved final scaled merged dataset")

"""## Verify and explore

### Subtask:
Verify and explore: Print the shape and head of the final merged dataset and generate summary statistics and visualizations to verify the data and explore its characteristics.

**Reasoning**:
Print the shape and head of the final merged dataset, then generate summary statistics and visualizations.
"""

print("Final merged dataset shape:", df_merged_daily.shape)
display(df_merged_daily.head())

print("\nSummary statistics:\n", df_merged_daily.describe().T)

num_cols = df_merged_daily.select_dtypes(include=np.number).columns
df_merged_daily[num_cols].hist(figsize=(15, 10))
plt.tight_layout()
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   Digital behavior data was loaded and processed from `app_usage_combined.csv` and `Behavior_combined.csv`. Daily features calculated included the number of app events, the total number of running tasks, and the count of behavior events.
*   Contextual data from `calendar_combined.csv` was loaded and processed, with daily aggregation calculating the number of calendar events per day.
*   No environmental data files were found in the specified directory. As a result, an empty dataframe with a 'date' column was created for this layer to ensure compatibility during merging.
*   The daily aggregated datasets from physiological, emotional (loaded from `merged_dataset.csv`), digital behavior, contextual, and environmental layers were successfully merged into a single dataframe (`df_merged_daily`) using outer joins on the date.
*   Missing values were present across several features in the merged dataset. The emotional feature columns (`mood_score`, `stress_level`, `energy_level`, `focus_level`) were dropped due to a high number of missing values. The remaining missing values in numeric columns were imputed using the median strategy.
*   The numeric columns of the final merged dataset were scaled using `StandardScaler`.
*   The final scaled dataset has a shape of (89, 41).

### Insights or Next Steps

*   Investigate columns with a standard deviation of 0 in the scaled dataset to understand why they have no variance and determine if they should be kept or removed.
*   Proceed with downstream analysis or modeling using the final scaled and integrated dataset, incorporating features from physiological, digital behavior, and contextual data layers.

"""

# Run in Colab: install required libs
!pip install -q tensorflow scikit-learn pandas matplotlib streamlit==1.26.0 shap

# Note: TF version can be adjusted for Colab runtime. Remove specific version if needed.

import os
import glob
import pandas as pd
import numpy as np
import datetime as dt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import joblib
import json
import shap

data_dir = "/content/processed"
# Load the final merged and scaled dataset
df = pd.read_csv(os.path.join(data_dir, "final_merged_scaled_dataset.csv"), parse_dates=['date'])
print("Loaded shape:", df.shape)
display(df.head())

# Ensure date column exists
if 'date' not in df.columns:
    raise ValueError("No 'date' column found. Ensure preprocessing produced dates.")

df = df.sort_values('date').reset_index(drop=True)

# Use 'date' column for daily snapshots
df['date_daily'] = df['date']

# Choose numeric sensor columns to aggregate (auto-detect)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
# drop any id-like columns you don't want aggregated
numeric_cols = [c for c in numeric_cols if c not in ('user_id','some_id')]

# Since data is already daily, no further aggregation needed
aggregated = df.copy()

print("Snapshot shape:", aggregated.shape)
display(aggregated.head())

# Features for autoencoder
X_snapshot = aggregated.drop(columns=['date', 'date_daily']).fillna(method='ffill').fillna(0)
feature_cols = X_snapshot.columns.tolist()

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_snapshot)

# Save scaler for later
os.makedirs('/content/models', exist_ok=True)
joblib.dump(scaler, '/content/models/snapshot_scaler.joblib')

# Train / val split
X_train, X_val = train_test_split(X_scaled, test_size=0.15, random_state=42, shuffle=True)
print("Train shape:", X_train.shape, "Val shape:", X_val.shape)

input_dim = X_train.shape[1]
latent_dim = min(32, max(8, input_dim // 2))  # heuristic; tune later

inputs = keras.Input(shape=(input_dim,))
x = layers.Dense(128, activation='relu')(inputs)
x = layers.Dense(64, activation='relu')(x)
z = layers.Dense(latent_dim, activation='linear', name='latent')(x)  # capsule
x = layers.Dense(64, activation='relu')(z)
x = layers.Dense(128, activation='relu')(x)
outputs = layers.Dense(input_dim, activation='linear')(x)

autoencoder = keras.Model(inputs, outputs, name='ae_dense')
encoder = keras.Model(inputs, z, name='encoder')

autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()

# Train
history = autoencoder.fit(
    X_train, X_train,
    validation_data=(X_val, X_val),
    epochs=60,
    batch_size=64,
    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],
    verbose=2
)
# Save models
autoencoder.save('/content/models/autoencoder.h5')
encoder.save('/content/models/encoder.h5')

# Encode all snapshots into capsules
Z = encoder.predict(X_scaled)  # shape: (n_snapshots, latent_dim)
capsules_df = pd.DataFrame(Z, columns=[f'z_{i}' for i in range(Z.shape[1])])
capsules_df['date_daily'] = aggregated['date_daily'].values
capsules_df = capsules_df[['date_daily'] + [c for c in capsules_df.columns if c.startswith('z_')]]
display(capsules_df.head())

# Save capsules
capsules_df.to_csv('/content/models/capsules.csv', index=False)
print("Saved capsules to /content/models/capsules.csv")

# Choose a target. If you have a real column, align; else we can use a proxy, e.g., next-step average HR change
# Example synthetic target: next-hour mean HR change normalised (replace with your real target column)
if 'heartrate_mean' in aggregated.columns:
    target_series = aggregated['heartrate_mean'].shift(-1).fillna(method='ffill')  # next hour hr_mean
else:
    # fallback: mean of first numeric column as proxy
    target_series = aggregated[feature_cols[0]].shift(-1).fillna(method='ffill')

# Prepare sequences from Z
seq_len = 24  # e.g., last 24 snapshots (hours)
X_seq = []
y_seq = []
timestamps_seq = []

Z_arr = Z  # numpy capsules
for i in range(len(Z_arr) - seq_len):
    X_seq.append(Z_arr[i:i+seq_len])
    y_seq.append(target_series.values[i+seq_len])  # target at next time
    timestamps_seq.append(aggregated['date_daily'].values[i+seq_len])

X_seq = np.array(X_seq)
y_seq = np.array(y_seq).reshape(-1,1)
print("X_seq shape:", X_seq.shape, "y shape:", y_seq.shape)

# Split train/test
X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(
    X_seq, y_seq, timestamps_seq, test_size=0.15, random_state=42, shuffle=True)

# scale targets
y_scaler = MinMaxScaler()
y_train_scaled = y_scaler.fit_transform(y_train)
y_test_scaled = y_scaler.transform(y_test)
joblib.dump(y_scaler, '/content/models/target_scaler.joblib')

timesteps = X_train.shape[1]
features = X_train.shape[2]

lstm_in = keras.Input(shape=(timesteps, features))
x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(lstm_in)
x = layers.Dense(32, activation='relu')(x)
out = layers.Dense(1, activation='linear')(x)

lstm_model = keras.Model(lstm_in, out, name='lstm_predictor')
lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
lstm_model.summary()

history_lstm = lstm_model.fit(
    X_train, y_train_scaled,
    validation_data=(X_test, y_test_scaled),
    epochs=80,
    batch_size=32,
    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],
    verbose=2
)
lstm_model.save('/content/models/lstm_model.h5')

pred_scaled = lstm_model.predict(X_test)
pred = y_scaler.inverse_transform(pred_scaled)
from sklearn.metrics import mean_squared_error, mean_absolute_error
mse = mean_squared_error(y_test, pred)
mae = mean_absolute_error(y_test, pred)
print("Eval MSE:", mse, "MAE:", mae)

plt.figure(figsize=(10,4))
plt.plot(range(len(y_test[:200])), y_test[:200], label='actual')
plt.plot(range(len(pred[:200])), pred[:200], label='pred')
plt.legend()
plt.title('Actual vs Pred (test sample)')
plt.show()

def permutation_importance(model, X_val, y_val_scaled, baseline_loss=None, n_repeats=8):
    if baseline_loss is None:
        baseline_loss = model.evaluate(X_val, y_val_scaled, verbose=0)[0]
    importances = []
    for dim in range(X_val.shape[2]):
        losses = []
        X_copy = X_val.copy()
        for _ in range(n_repeats):
            # permute this dimension across samples/timesteps
            perm = np.random.permutation(X_copy[:,:,dim].ravel()).reshape(X_copy.shape[0], X_copy.shape[1])
            X_copy_perm = X_copy.copy()
            X_copy_perm[:,:,dim] = perm
            loss = model.evaluate(X_copy_perm, y_val_scaled, verbose=0)[0]
            losses.append(loss)
        importances.append(np.mean(losses) - baseline_loss)
    return np.array(importances)

baseline = lstm_model.evaluate(X_test, y_test_scaled, verbose=0)[0]
importances = permutation_importance(lstm_model, X_test, y_test_scaled, baseline_loss=baseline)
# show top dims
idx = np.argsort(importances)[::-1]
print("Top latent dims (by permutation importance):", idx[:10])
plt.bar(range(len(importances)), importances[idx])
plt.title("Permutation importances of latent dims (sorted)")
plt.show()

# Load saved artifacts
encoder = keras.models.load_model('/content/models/encoder.h5', compile=False) # Don't need to compile encoder for prediction
scaler = joblib.load('/content/models/snapshot_scaler.joblib')
y_scaler = joblib.load('/content/models/target_scaler.joblib')

# Redefine the LSTM model architecture
timesteps = X_train.shape[1]
features = X_train.shape[2]

lstm_in = keras.Input(shape=(timesteps, features))
x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(lstm_in)
x = layers.Dense(32, activation='relu')(x)
out = layers.Dense(1, activation='linear')(x)

lstm_model = keras.Model(lstm_in, out, name='lstm_predictor')

# Load the weights from the saved model file
lstm_model.load_weights('/content/models/lstm_model.h5')

# Compile the model (optional, but good practice if you plan to continue training)
lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])


def predict_next(snapshot_row_df, last_seq_capsules):
    # snapshot_row_df: single-row DataFrame of current features (unscaled)
    # last_seq_capsules: numpy array shaped (seq_len, latent_dim)
    x_scaled = scaler.transform(snapshot_row_df[feature_cols].values)
    z_new = encoder.predict(x_scaled.reshape(1,-1))  # new capsule
    seq = np.vstack([last_seq_capsules[1:], z_new])  # shift window
    seq = seq.reshape(1, seq.shape[0], seq.shape[1])
    pred_scaled = lstm_model.predict(seq)
    pred = y_scaler.inverse_transform(pred_scaled)[0,0]
    return pred, z_new

def simple_policy(pred_value, thresholds=(0.1, 0.3)):
    # thresholds depend on your target scale — adapt accordingly.
    if pred_value < thresholds[0]:
        action = {"intent":"encourage_rest","message":"Looks like you're doing well. Keep the flow!"}
    elif pred_value < thresholds[1]:
        action = {"intent":"gentle_nudge","message":"Your recent signals show slight risk — consider a 5-min break."}
    else:
        action = {"intent":"assertive_nudge","message":"High risk predicted — please take a longer break, step away from screen and breathe."}
    return action

# Example usage (needs last_seq_capsules prepared)
# pred, capsule = predict_next(aggregated.iloc[-1:].copy(), Z[-seq_len:])
# action = simple_policy(pred)
# print(pred, action)

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py <<'PY'
# import streamlit as st
# import pandas as pd, numpy as np, joblib, keras
# from datetime import datetime
# st.set_page_config(layout="wide", page_title="NTC Demo")
# 
# # load artifacts
# encoder = keras.models.load_model('/content/models/encoder.h5')
# lstm = keras.models.load_model('/content/models/lstm_model.h5')
# scaler = joblib.load('/content/models/snapshot_scaler.joblib')
# y_scaler = joblib.load('/content/models/target_scaler.joblib')
# capsules = pd.read_csv('/content/models/capsules.csv', parse_dates=['timestamp'])
# 
# st.title("Neural Time Capsule — Demo")
# st.write("Timeline of latent capsules and next-step prediction demo.")
# 
# # timeline plot
# st.line_chart(capsules.filter(like='z_').iloc[-200:])
# 
# # quick prediction form
# st.sidebar.header("Predict next")
# # choose last N rows to form seq
# seq_len = 24
# if st.button("Predict using last sequence"):
#     Z = capsules.filter(like='z_').iloc[-(seq_len):].values
#     seq = Z.reshape(1, Z.shape[0], Z.shape[1])
#     pred_scaled = lstm.predict(seq)
#     pred = y_scaler.inverse_transform(pred_scaled)[0,0]
#     st.sidebar.success(f"Predicted next target: {pred:.3f}")
#     # simple policy
#     if pred > 0.3:
#         st.warning("Agent action: assertive nudge — take a break!")
#     elif pred > 0.1:
#         st.info("Agent action: gentle nudge — 5-min break recommended.")
#     else:
#         st.write("All good. Keep going!")
# PY
# echo "Created app.py. Run with: streamlit run app.py"
#

# Make predictions on the test set
pred_scaled_test = lstm_model.predict(X_test)
pred_test = y_scaler.inverse_transform(pred_scaled_test)

# Inverse scale the actual test values for plotting
y_test_unscaled = y_scaler.inverse_transform(y_test)

# Prepare data for policy application on the test set
# We need the original unscaled features for the 'other_features_data' argument
# This requires aligning the test set dates (t_test) with the original 'aggregated' dataframe
test_dates_df = pd.DataFrame({'date_daily': t_test})
merged_test_data = test_dates_df.merge(aggregated, on='date_daily', how='left')

# Apply the combined policy agent to each prediction in the test set
test_actions = []
other_features = ['num_app_events', 'num_running_tasks', 'num_behavior_events', 'num_calendar_events'] # Defined previously
for i in range(len(pred_test)):
    # Get other features for the current test date
    current_date_data = merged_test_data.iloc[i]
    current_other_features_data = current_date_data[other_features]

    # Get recommendation using the combined policy
    action = combined_policy_agent(pred_test[i][0], current_other_features_data, policy_thresholds)
    test_actions.append(action)

# Extract dates for plotting
plot_dates = t_test

# Plotting
plt.figure(figsize=(15, 7))
plt.plot(plot_dates, y_test_unscaled, label='Actual Target', marker='o', linestyle='-', markersize=4)
plt.plot(plot_dates, pred_test, label='Predicted Target', marker='x', linestyle='--', markersize=4)

# Overlay agent actions as annotations or markers
# Define markers/colors for different intents
intent_markers = {
    "minimal_intervention": 'o',
    "gentle_nudge": '^',
    "assertive_nudge": 'v'
}
intent_colors = {
    "minimal_intervention": 'blue',
    "gentle_nudge": 'orange',
    "assertive_nudge": 'red'
}

for i, action in enumerate(test_actions):
    # Safely get the intent, defaulting to 'unknown' if not present
    intent = action.get('original_intent', 'unknown')
    date = plot_dates[i]
    # You might want to plot the marker at the actual value or predicted value, or just on the date axis
    # Here, let's plot it at the actual value for that date for context
    y_value = y_test_unscaled[i][0]

    plt.plot(date, y_value,
             marker=intent_markers.get(intent, 'x'),
             color=intent_colors.get(intent, 'gray'),
             markersize=10,
             label=f'{intent} Action' if i == 0 else "", # Label once for legend
             alpha=0.7)

plt.title('Actual vs Predicted Target with Agent Actions (Test Set)')
plt.xlabel('Date')
plt.ylabel('Target Value (Unscaled)')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Ensure feature_cols, aggregated, Z, seq_len, predict_next, combined_policy_agent, and policy_thresholds are defined
if 'feature_cols' not in globals() or 'aggregated' not in globals() or 'Z' not in globals() or 'seq_len' not in globals() or 'predict_next' not in globals() or 'combined_policy_agent' not in globals() or 'policy_thresholds' not in globals():
    print("Required variables not found. Please ensure previous cells have been run.")
else:
    # Get the latest snapshot and the last sequence of capsules
    latest_snapshot_df = aggregated.iloc[-1:].copy()
    last_seq_capsules = Z[-seq_len:]

    # Make a prediction using the trained model
    predicted_target, new_capsule = predict_next(latest_snapshot_df, last_seq_capsules)

    # Get other features data for the latest snapshot to pass to the dynamic policy
    # We'll use the other_features list defined previously for correlation analysis
    other_features = ['num_app_events', 'num_running_tasks', 'num_behavior_events', 'num_calendar_events']
    latest_other_features_data = latest_snapshot_df.iloc[0][other_features]

    # Get the recommended action using the combined policy and advanced agent logic
    # You can add additional_context here if needed, e.g., time of day
    recommended_action = combined_policy_agent(predicted_target, latest_other_features_data, policy_thresholds)

    print(f"Predicted next target value: {predicted_target:.3f}")
    print("Recommended action (from Advanced Agent Logic):", recommended_action)

    # Optional: Add the new capsule to the capsules_df for future use
    # Note: This requires determining the date for the new prediction.
    # If predicting for the next day, you would determine that date.
    # new_capsule_df = pd.DataFrame(new_capsule, columns=[f'z_{i}' for i in range(new_capsule.shape[1])])
    # Assuming the next date is simply the last date + 1 day
    # last_date = pd.to_datetime(capsules_df['date_daily'].iloc[-1])
    # next_date = last_date + pd.Timedelta(days=1)
    # new_capsule_df['date_daily'] = next_date
    # capsules_df = pd.concat([capsules_df, new_capsule_df], ignore_index=True)
    # print("\nUpdated capsules_df shape:", capsules_df.shape)

import pandas as pd

# Load the final merged and scaled dataset
df_final_scaled = pd.read_csv("/content/processed/final_merged_scaled_dataset.csv")

print("Head of the final cleaned and scaled dataset:")
display(df_final_scaled.head())

def dynamic_policy(predicted_target, other_features_data, thresholds):
    """
    Develops a dynamic recommendation policy based on predicted target and other features.

    Args:
        predicted_target (float): The predicted value of the target variable.
        other_features_data (pd.Series): A Series containing values of other relevant features
                                         for the current day (e.g., num_behavior_events).
        thresholds (dict): A dictionary of thresholds for different risk levels.

    Returns:
        dict: A dictionary containing the recommended 'intent' and 'message'.
    """
    # Example of a more dynamic policy:
    # Combine predicted target with another feature, e.g., num_behavior_events
    # Note: This is a simplified example; a real-world policy might use
    # more complex logic, learned rules, or even another model.

    behavior_impact = other_features_data.get('num_behavior_events', 0) # Get value, default to 0 if not present

    # Adjust the predicted target based on the behavior impact (example logic)
    # Let's assume higher behavior events might indicate more activity, potentially
    # influencing the interpretation of heart rate.
    # This is a placeholder - the actual relationship would need to be determined
    # from domain knowledge or further analysis.
    adjusted_risk_score = predicted_target + (behavior_impact * 0.1) # Example adjustment

    if adjusted_risk_score > thresholds['high_risk']:
        action = {"intent": "assertive_nudge", "message": "High risk predicted based on combined signals — please take a longer break, step away from screen and breathe."}
    elif adjusted_risk_score > thresholds['moderate_risk']:
        action = {"intent": "gentle_nudge", "message": "Your recent signals show slight risk — consider a 5-min break."}
    else:
        action = {"intent": "minimal_intervention", "message": "Looks like you're doing well. Keep the flow!"}

    return action

# Define example thresholds (these should be determined based on the scaled target distribution and desired sensitivity)
# These are placeholders and need to be tuned based on the actual data and target variable.
policy_thresholds = {
    'high_risk': 1.0,      # Example: if adjusted risk score is above 1 standard deviation
    'moderate_risk': 0.5   # Example: if adjusted risk score is above 0.5 standard deviation
}

# To test the policy, you would need the predicted target and other features for a specific day.
# This would typically come from the prediction step using the LSTM model.

# Example of how you would get data for a specific day (assuming you have a date):
# test_date = pd.to_datetime('2025-10-04').date() # Example date from the dataset
# latest_snapshot_for_date = merged_for_analysis[merged_for_analysis['date'] == test_date].iloc[0]
# predicted_target_for_date = latest_snapshot_for_date[target_col] # Replace with actual prediction
# other_features_for_date = latest_snapshot_for_date[other_features]
# recommended_action = dynamic_policy(predicted_target_for_date, other_features_for_date, policy_thresholds)
# print(f"Policy recommendation for {test_date}: {recommended_action}")

print("Dynamic policy function defined.")

# Merge scaled features and capsules on date
merged_for_analysis = df_scaled.merge(df_capsules, left_on='date', right_on='date_daily', how='inner')

# Identify the target column (using heartrate_mean as specified in previous cells as the target proxy)
target_col = 'heartrate_mean'

# Analyze correlations between latent dimensions and the target
capsule_cols = [c for c in merged_for_analysis.columns if c.startswith('z_')]
correlations_z_target = merged_for_analysis[capsule_cols + [target_col]].corr()[target_col].drop(target_col)

print("\nCorrelations between latent dimensions and target:")
print(correlations_z_target.sort_values(ascending=False))

# Also look at correlations between other features and the target
other_features = ['num_app_events', 'num_running_tasks', 'num_behavior_events', 'num_calendar_events']
correlations_other_target = merged_for_analysis[other_features + [target_col]].corr()[target_col].drop(target_col)

print("\nCorrelations between other features and target:")
print(correlations_other_target.sort_values(ascending=False))

"""## Refine the policy

### Subtask:
Develop a more dynamic and personalized policy for generating recommendations based on factors beyond simple thresholds.
"""

# Ensure feature_cols is defined (from previous steps)
if 'feature_cols' not in globals():
    print("feature_cols not found. Please re-run previous cells to define it.")
else:
    # Get the latest snapshot and the last sequence of capsules
    latest_snapshot_df = aggregated.iloc[-1:].copy()
    last_seq_capsules = Z[-seq_len:]

    # Make a prediction
    pred, new_capsule = predict_next(latest_snapshot_df, last_seq_capsules)

    # Get the recommended action based on the prediction
    action = simple_policy(pred)

    print(f"Predicted next target value: {pred:.3f}")
    print("Recommended action:", action)

    # Optional: Add the new capsule to the capsules_df for future use
    # Note: This requires knowing the date for the new prediction.
    # If predicting for the next day, you would determine that date.
    # new_capsule_df = pd.DataFrame(new_capsule, columns=[f'z_{i}' for i in range(new_capsule.shape[1])])
    # Assuming the next date is simply the last date + 1 day
    # last_date = pd.to_datetime(capsules_df['date_daily'].iloc[-1])
    # next_date = last_date + pd.Timedelta(days=1)
    # new_capsule_df['date_daily'] = next_date
    # capsules_df = pd.concat([capsules_df, new_capsule_df], ignore_index=True)
    # print("\nUpdated capsules_df shape:", capsules_df.shape)

"""# Task
Enhance the Neural Time Capsule project by refining the recommendation policy, developing advanced agent logic, visualizing predictions and actions, implementing a feedback mechanism, and integrating these enhancements into the existing codebase.

## Refine the policy

### Subtask:
Develop a more dynamic and personalized policy for generating recommendations based on factors beyond simple thresholds.

**Reasoning**:
Load the necessary dataframes and inspect their columns to understand the available features for developing a more dynamic policy.
"""

df_scaled = pd.read_csv("/content/processed/final_merged_scaled_dataset.csv", parse_dates=['date'])
df_capsules = pd.read_csv("/content/models/capsules.csv", parse_dates=['date_daily'])

print("Scaled dataset columns:", df_scaled.columns)
print("\nCapsules dataset columns:", df_capsules.columns)

display(df_scaled.head())
display(df_capsules.head())

"""**Reasoning**:
Analyze the relationships between latent space features and the target variable and other relevant features using correlation analysis to inform policy development.


"""

# Merge scaled features and capsules on date
merged_for_analysis = df_scaled.merge(df_capsules, left_on='date', right_on='date_daily', how='inner')

# Identify the target column (using heartrate_mean as specified in previous cells as the target proxy)
target_col = 'heartrate_mean'

# Analyze correlations between latent dimensions and the target
capsule_cols = [c for c in merged_for_analysis.columns if c.startswith('z_')]
correlations_z_target = merged_for_analysis[capsule_cols + [target_col]].corr()[target_col].drop(target_col)

print("\nCorrelations between latent dimensions and target:")
print(correlations_z_target.sort_values(ascending=False))

# Also look at correlations between other features and the target
other_features = ['num_app_events', 'num_running_tasks', 'num_behavior_events', 'num_calendar_events']
correlations_other_target = merged_for_analysis[other_features + [target_col]].corr()[target_col].drop(target_col)

print("\nCorrelations between other features and target:")
print(correlations_other_target.sort_values(ascending=False))

"""**Reasoning**:
Define a more dynamic policy function that considers the predicted target value and potentially other relevant features identified in the correlation analysis to provide nuanced recommendations.


"""

def dynamic_policy(predicted_target, other_features_data, thresholds):
    """
    Develops a dynamic recommendation policy based on predicted target and other features.

    Args:
        predicted_target (float): The predicted value of the target variable.
        other_features_data (pd.Series): A Series containing values of other relevant features
                                         for the current day (e.g., num_behavior_events).
        thresholds (dict): A dictionary of thresholds for different risk levels.

    Returns:
        dict: A dictionary containing the recommended 'intent' and 'message'.
    """
    # Example of a more dynamic policy:
    # Combine predicted target with another feature, e.g., num_behavior_events
    # Note: This is a simplified example; a real-world policy might use
    # more complex logic, learned rules, or even another model.

    behavior_impact = other_features_data.get('num_behavior_events', 0) # Get value, default to 0 if not present

    # Adjust the predicted target based on the behavior impact (example logic)
    # Let's assume higher behavior events might indicate more activity, potentially
    # influencing the interpretation of heart rate.
    # This is a placeholder - the actual relationship would need to be determined
    # from domain knowledge or further analysis.
    adjusted_risk_score = predicted_target + (behavior_impact * 0.1) # Example adjustment

    if adjusted_risk_score > thresholds['high_risk']:
        action = {"intent": "assertive_nudge", "message": "High risk predicted based on combined signals — please take a longer break, step away from screen and breathe."}
    elif adjusted_risk_score > thresholds['moderate_risk']:
        action = {"intent": "gentle_nudge", "message": "Your recent signals show slight risk — consider a 5-min break."}
    else:
        action = {"intent": "minimal_intervention", "message": "Looks like you're doing well. Keep the flow!"}

    return action

# Define example thresholds (these should be determined based on the scaled target distribution and desired sensitivity)
# These are placeholders and need to be tuned based on the actual data and target variable.
policy_thresholds = {
    'high_risk': 1.0,      # Example: if adjusted risk score is above 1 standard deviation
    'moderate_risk': 0.5   # Example: if adjusted risk score is above 0.5 standard deviation
}

# To test the policy, you would need the predicted target and other features for a specific day.
# This would typically come from the prediction step using the LSTM model.

# Example of how you would get data for a specific day (assuming you have a date):
# test_date = pd.to_datetime('2025-10-04').date() # Example date from the dataset
# latest_snapshot_for_date = merged_for_analysis[merged_for_analysis['date'] == test_date].iloc[0]
# predicted_target_for_date = latest_snapshot_for_date[target_col] # Replace with actual prediction
# other_features_for_date = latest_snapshot_for_date[other_features]
# recommended_action = dynamic_policy(predicted_target_for_date, other_features_for_date, policy_thresholds)
# print(f"Policy recommendation for {test_date}: {recommended_action}")

print("Dynamic policy function defined.")

"""## Develop advanced agent logic

### Subtask:
Implement more complex agent behavior to provide nuanced suggestions and potentially integrate with other tools.

**Reasoning**:
Define the `advanced_agent_logic` function to provide more nuanced suggestions based on the dynamic policy output and update the `simple_policy` function to use the dynamic policy and the advanced agent logic.
"""

def advanced_agent_logic(dynamic_policy_recommendation, additional_context=None):
    """
    Provides more nuanced suggestions based on the dynamic policy recommendation.

    Args:
        dynamic_policy_recommendation (dict): The output from the dynamic_policy function,
                                             containing 'intent' and 'message'.
        additional_context (dict, optional): Additional information for personalization
                                             (e.g., time of day, recent activity). Defaults to None.

    Returns:
        dict: A dictionary containing the refined agent's suggestion.
    """
    intent = dynamic_policy_recommendation.get('intent')
    message = dynamic_policy_recommendation.get('message')
    refined_suggestion = {"original_intent": intent, "original_message": message, "suggestion": message} # Start with original message

    if intent == "gentle_nudge":
        refined_suggestion["suggestion"] = f"Gentle nudge: {message} Consider a short walk, stretching, or listening to calming music for 5 minutes."
    elif intent == "assertive_nudge":
        refined_suggestion["suggestion"] = f"Assertive nudge: {message} Try a longer break (15-20 mins). Step away from screens, practice deep breathing, or do a quick mindfulness exercise."
    elif intent == "minimal_intervention":
         refined_suggestion["suggestion"] = f"Minimal intervention: {message} You seem to be in a good state. Keep up the good habits, but remember to take breaks if you feel tired."

    # Example of incorporating additional context (placeholder)
    if additional_context and additional_context.get("time_of_day") == "evening" and intent != "assertive_nudge":
         refined_suggestion["suggestion"] += " Consider winding down soon."


    return refined_suggestion

def combined_policy_agent(predicted_target, other_features_data, thresholds, additional_context=None):
    """
    Combines the dynamic policy and advanced agent logic to provide a final suggestion.

    Args:
        predicted_target (float): The predicted value of the target variable.
        other_features_data (pd.Series): A Series containing values of other relevant features.
        thresholds (dict): A dictionary of thresholds for different risk levels.
        additional_context (dict, optional): Additional information for personalization. Defaults to None.

    Returns:
        dict: The final, nuanced agent's suggestion.
    """
    dynamic_rec = dynamic_policy(predicted_target, other_features_data, thresholds)
    final_suggestion = advanced_agent_logic(dynamic_rec, additional_context)
    return final_suggestion

# Test the advanced_agent_logic with example inputs
example_policy_rec_gentle = {"intent": "gentle_nudge", "message": "Your recent signals show slight risk — consider a 5-min break."}
example_policy_rec_assertive = {"intent": "assertive_nudge", "message": "High risk predicted based on combined signals — please take a longer break, step away from screen and breathe."}
example_policy_rec_minimal = {"intent": "minimal_intervention", "message": "Looks like you're doing well. Keep the flow!"}

print("Testing advanced_agent_logic:")
print("Gentle Nudge:", advanced_agent_logic(example_policy_rec_gentle))
print("Assertive Nudge:", advanced_agent_logic(example_policy_rec_assertive))
print("Minimal Intervention:", advanced_agent_logic(example_policy_rec_minimal))

# Example with context
example_context = {"time_of_day": "evening"}
print("\nTesting advanced_agent_logic with context:")
print("Gentle Nudge (Evening):", advanced_agent_logic(example_policy_rec_gentle, example_context))

# Evaluation & Diagnostics cell
import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

os.makedirs("/content/analysis_plots", exist_ok=True)

# Load artifacts if not in memory
try:
    lstm_model
    X_test
    y_test
    y_scaler
except NameError:
    from tensorflow import keras
    lstm_model = keras.models.load_model('/content/models/lstm_model.h5', compile=False)
    X_test = np.load('/content/models/X_test.npy') if os.path.exists('/content/models/X_test.npy') else X_test
    y_test = np.load('/content/models/y_test.npy') if os.path.exists('/content/models/y_test.npy') else y_test
    y_scaler = joblib.load('/content/models/target_scaler.joblib')

# Predict
pred_scaled = lstm_model.predict(X_test)
pred = y_scaler.inverse_transform(pred_scaled.reshape(-1,1)).reshape(-1)
y_true = y_test.reshape(-1)

# Metrics
mse = mean_squared_error(y_true, pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true, pred)
r2 = r2_score(y_true, pred)
print("Eval metrics -> MSE: {:.6f}, RMSE: {:.6f}, MAE: {:.6f}, R2: {:.4f}".format(mse, rmse, mae, r2))

# Residuals and distribution
residuals = y_true - pred

plt.figure(figsize=(10,4))
plt.plot(y_true[:200], label='actual')
plt.plot(pred[:200], label='pred')
plt.title('Actual vs Pred (first 200 samples)')
plt.legend()
plt.savefig('/content/analysis_plots/actual_vs_pred.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,4))
plt.hist(residuals, bins=50)
plt.title('Residual distribution')
plt.savefig('/content/analysis_plots/residual_hist.png', bbox_inches='tight')
plt.show()

# Residual vs predicted
plt.figure(figsize=(7,5))
plt.scatter(pred, residuals, alpha=0.5)
plt.axhline(0, color='k', linestyle='--')
plt.xlabel('Predicted')
plt.ylabel('Residual (true - pred)')
plt.title('Residuals vs Predicted')
plt.savefig('/content/analysis_plots/residuals_vs_pred.png', bbox_inches='tight')
plt.show()

# Permutation importance already implemented earlier; recompute if needed
def permutation_importance(model, X_val, y_val_scaled, baseline_loss=None, n_repeats=8):
    if baseline_loss is None:
        baseline_loss = model.evaluate(X_val, y_val_scaled, verbose=0)[0]
    importances = []
    for dim in range(X_val.shape[2]):
        losses = []
        X_copy = X_val.copy()
        for _ in range(n_repeats):
            perm = np.random.permutation(X_copy[:,:,dim].ravel()).reshape(X_copy.shape[0], X_copy.shape[1])
            X_copy_perm = X_copy.copy()
            X_copy_perm[:,:,dim] = perm
            loss = model.evaluate(X_copy_perm, y_val_scaled, verbose=0)[0]
            losses.append(loss)
        importances.append(np.mean(losses) - baseline_loss)
    return np.array(importances)

# Recompute baseline on test
from sklearn.preprocessing import MinMaxScaler
# If y_test is not scaled yet, scale it to compute loss consistently
if os.path.exists('/content/models/target_scaler.joblib'):
    y_scaler = joblib.load('/content/models/target_scaler.joblib')
    y_test_scaled = y_scaler.transform(y_test.reshape(-1,1))
else:
    y_test_scaled = y_test

baseline = lstm_model.evaluate(X_test, y_test_scaled, verbose=0)[0]
importances = permutation_importance(lstm_model, X_test, y_test_scaled, baseline_loss=baseline, n_repeats=6)
idx = np.argsort(importances)[::-1]

# Plot top 12 latent dims
topk = min(12, len(importances))
plt.figure(figsize=(10,4))
plt.bar(range(topk), importances[idx][:topk])
plt.xticks(range(topk), [f"z_{i}" for i in idx[:topk]])
plt.ylabel("Increase in loss (importance)")
plt.title("Top latent dims by permutation importance")
plt.savefig('/content/analysis_plots/top_latent_importances.png', bbox_inches='tight')
plt.show()

# Save numeric importance CSV
pd.DataFrame({'dim': [f"z_{i}" for i in range(len(importances))], 'importance': importances}).sort_values('importance', ascending=False).to_csv('/content/analysis_plots/latent_importances.csv', index=False)

print("Saved plots and latent_importances.csv in /content/analysis_plots")

# Emotional imputation combo: KNN + Autoencoder
import os, pandas as pd, numpy as np
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers

emotion_path = "/content/processed/emotion_features.csv"
out_dir = "/content/processed"
os.makedirs(out_dir, exist_ok=True)

df_em = pd.read_csv(emotion_path)
print("Original emotion shape:", df_em.shape)
# Mark rows with missing values
missing_summary = df_em.isna().sum()
print("Missing per column:\n", missing_summary)

# Option A: KNN imputer
knn = KNNImputer(n_neighbors=5)
knn_imputed_file = os.path.join(out_dir, "emotion_imputed_knn.csv")
try:
    em_knn = pd.DataFrame(knn.fit_transform(df_em), columns=df_em.columns)
    em_knn.to_csv(knn_imputed_file, index=False)
    print("Saved emotion_imputed_knn.csv")
except Exception as e:
    print("KNN imputer failed:", e)
    em_knn = None # Set to None if imputation failed

# Option B: Autoencoder-based imputation
#  - strategy: temporary fill with column medians, train AE to reconstruct complete rows, then replace original missing with AE outputs
ae_df = df_em.copy()
median_vals = ae_df.median()
# Check if median_vals contains any non-numeric types after calculating median
if not pd.api.types.is_numeric_dtype(median_vals):
    print("Warning: Median values contain non-numeric types. Skipping Autoencoder imputation.")
    df_em_ae_imputed = None
else:
    ae_df_filled = ae_df.fillna(median_vals)

    # Standardize for AE
    scaler = StandardScaler()
    X = scaler.fit_transform(ae_df_filled)

    # small autoencoder
    input_dim = X.shape[1]
    latent = max(2, input_dim//3)
    inp = keras.Input(shape=(input_dim,))
    h = layers.Dense(max(16,input_dim//2), activation='relu')(inp)
    z = layers.Dense(latent, activation='linear', name='latent')(h)
    h2 = layers.Dense(max(16,input_dim//2), activation='relu')(z)
    out = layers.Dense(input_dim, activation='linear')(h2)
    ae = keras.Model(inp, out)
    ae.compile(optimizer='adam', loss='mse')
    ae.fit(X, X, epochs=80, batch_size=16, verbose=0, callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=8, restore_best_weights=True)])

    # Predict reconstructed values
    X_rec = ae.predict(X)
    X_rec_inv = scaler.inverse_transform(X_rec)
    df_rec = pd.DataFrame(X_rec_inv, columns=df_em.columns)

    # Build final imputed df: where original was NaN, take reconstructed value; else keep original
    df_em_ae_imputed = df_em.copy()
    for c in df_em.columns:
        mask = df_em[c].isna()
        df_em_ae_imputed.loc[mask, c] = df_rec.loc[mask, c]

    df_em_ae_imputed.to_csv(os.path.join(out_dir, "emotion_imputed_ae.csv"), index=False)
    print("Saved emotion_imputed_ae.csv")


# Quick compare of missing values after imputation
if em_knn is not None:
    print("KNN missing after:", em_knn.isna().sum().sum())
else:
    print("KNN imputation failed, skipping missing value count.")

if df_em_ae_imputed is not None:
    print("AE  missing after:", df_em_ae_imputed.isna().sum().sum())
else:
    print("AE imputation skipped or failed, skipping missing value count.")

# Feedback logger: append feedback to CSV
import csv, os, time, datetime as dt, json

LOG_DIR = "/content/feedback"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "feedback_log.csv")

# Ensure header exists
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp_utc","user_id","date","prediction","action_intent","action_msg","user_response","response_metadata"])

def log_feedback(user_id, date, prediction, action_intent, action_msg, user_response, response_metadata=None):
    """
    Append a feedback record to CSV.
    - user_id: string or int
    - date: the date the prediction/action was for (YYYY-MM-DD or datetime.date)
    - prediction: float
    - action_intent: string
    - action_msg: string
    - user_response: string (accepted/ignored/snoozed/custom)
    - response_metadata: dict (optional) e.g. {"duration":300,"notes":"felt better"}
    """
    ts = dt.datetime.utcnow().isoformat()
    meta_json = json.dumps(response_metadata or {})
    with open(LOG_FILE, 'a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([ts, user_id, str(date), float(prediction), action_intent, action_msg, user_response, meta_json])
    return True

# Example usage
log_feedback(user_id="test_user", date="2025-10-04", prediction=0.45, action_intent="gentle_nudge",
             action_msg="Take a 5-min walk", user_response="accepted", response_metadata={"duration":300})
print("Feedback appended to", LOG_FILE)

import matplotlib.pyplot as plt

# Select some key metrics to plot
metrics_to_plot = ['heartrate_mean', 'skin_temperature_mean', 'num_app_events', 'num_behavior_events', 'num_calendar_events']

# Ensure selected metrics are in the aggregated dataframe
available_metrics = [m for m in metrics_to_plot if m in aggregated.columns]
if not available_metrics:
    print("None of the selected metrics are available in the aggregated dataframe.")
else:
    plt.figure(figsize=(15, 10))

    for i, metric in enumerate(available_metrics):
        plt.subplot(len(available_metrics), 1, i + 1)
        plt.plot(aggregated['date_daily'], aggregated[metric])
        plt.title(f'{metric} over Time')
        plt.ylabel(metric)
        plt.grid(True)

    plt.xlabel('Date')
    plt.tight_layout()
    plt.show()

"""### Outline: Visualizations for Integrated Time Series Data

To create outstanding visualizations for the integrated time series data, we can focus on clarity, insightfulness, and highlighting the agent's interventions.

**Key Visualization Components:**

1.  **Multi-Metric Timeline:** A single timeline displaying multiple relevant time series features together. This could include:
    *   Predicted target variable (e.g., 'heartrate_mean')
    *   Other key physiological metrics (e.g., 'skin_temperature_mean', 'spo2_mean')
    *   Relevant digital behavior metrics (e.g., 'num_app_events', 'num_behavior_events')
    *   Contextual information (e.g., 'num_calendar_events')

2.  **Agent Intervention Markers:** Overlay markers or annotations directly on the timeline to indicate when and what type of recommendation the agent provided. Different marker styles or colors can represent different intents (e.g., minimal intervention, gentle nudge, assertive nudge). Clicking on a marker in an interactive visualization could reveal the specific message.

3.  **Risk Level Highlighting:** Visually indicate periods of predicted high risk or deviation from baseline on the timeline, perhaps with shaded areas or background colors.

4.  **Correlation or Relationship Views (Optional but Unique):** Smaller linked plots or heatmaps that show correlations between different metrics over time, or how specific behaviors align with physiological changes.

**Making it Outstanding and Unique:**

*   **Focus on Narrative:** Design the visualizations to tell a story about the user's day or week, showing how different factors influence their state and how the agent attempts to guide them.
*   **Clear and Intuitive Design:** Avoid overly complex plots. Use clear labels, intuitive color schemes, and interactive elements (if building a separate UI) to make the data easily understandable.
*   **Personalized Insights:** If possible, tailor the visualizations to the individual user, perhaps highlighting patterns that are unique to them.
*   **Emphasis on Actionable Information:** Ensure the visualizations clearly show *why* an action was recommended and what the potential impact could be.

These visualizations would serve as a powerful tool for users to understand their patterns and the value of the Neural Time Capsule's guidance.

## Develop Enhanced Visualization and User Interface (Conceptual/Outline)

### Subtask:
Outline the conceptual design for enhanced visualizations and user interface elements to make the project stand out.
"""

# Feedback logger: append feedback to CSV
import csv, os, time, datetime as dt, json

LOG_DIR = "/content/feedback"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "feedback_log.csv")

# Ensure header exists
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp_utc","user_id","date","prediction","action_intent","action_msg","user_response","response_metadata"])

def log_feedback(user_id, date, prediction, action_intent, action_msg, user_response, response_metadata=None):
    """
    Append a feedback record to CSV.
    - user_id: string or int
    - date: the date the prediction/action was for (YYYY-MM-DD or datetime.date)
    - prediction: float
    - action_intent: string
    - action_msg: string
    - user_response: string (accepted/ignored/snoozed/custom)
    - response_metadata: dict (optional) e.g. {"duration":300,"notes":"felt better"}
    """
    ts = dt.datetime.utcnow().isoformat()
    meta_json = json.dumps(response_metadata or {})
    with open(LOG_FILE, 'a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([ts, user_id, str(date), float(prediction), action_intent, action_msg, user_response, meta_json])
    return True

# Example usage
log_feedback(user_id="test_user", date="2025-10-04", prediction=0.45, action_intent="gentle_nudge",
             action_msg="Take a 5-min walk", user_response="accepted", response_metadata={"duration":300})
print("Feedback appended to", LOG_FILE)